import requests
from bs4 import BeautifulSoup
import os
import re
import time
from collections import deque
from urllib.parse import urljoin, urlparse
import base64
import google.generativeai as genai
from dotenv import load_dotenv
from PIL import Image, ImageFile
import io

ImageFile.LOAD_TRUNCATED_IMAGES = True

# ----------------------------
# 1. API í‚¤ ë° ëª¨ë¸ ë¡œë“œ
# ----------------------------
load_dotenv()
API_KEY = os.environ.get("GOOGLE_API_KEY")
if not API_KEY:
    print("ì˜¤ë¥˜: .env íŒŒì¼ì— GOOGLE_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")
    exit()

genai.configure(api_key=API_KEY)

OCR_MODEL = genai.GenerativeModel('gemini-2.5-pro')
OCR_PROMPT = """
ì´ ì´ë¯¸ì§€ëŠ” ê³µì§€ì‚¬í•­ ë¬¸ì„œì…ë‹ˆë‹¤.
ì´ë¯¸ì§€ ìƒë‹¨ë¶€í„° í•˜ë‹¨ê¹Œì§€, ëˆˆì— ë³´ì´ëŠ” ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ìˆœì„œëŒ€ë¡œ ë¹ ì§ì—†ì´ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
ë°°ê²½ìƒ‰ì´ ë‹¤ë¥´ë”ë¼ë„ ëª¨ë“  ì˜ì—­ì˜ í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
ì„¤ëª…ì´ë‚˜ ìš”ì•½ ì—†ì´, ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì›ë³¸ë§Œ ì œê³µí•´ì£¼ì„¸ìš”.
"""


# ----------------------------
# 2. í…ìŠ¤íŠ¸ í´ë¦° í•¨ìˆ˜
# ----------------------------
def clean_text(text):
    text = re.sub(r'\n\s*\n', '\n', text)
    lines = [line.strip() for line in text.split('\n')]
    cleaned_lines = [line for line in lines if len(line) > 5]
    return "\n".join(cleaned_lines)


# ----------------------------
# 3. Gemini OCR í•¨ìˆ˜
# ----------------------------
def ocr_with_gemini(image_url, headers):
    try:
        image_content = None

        if image_url.startswith('data:image'):
            print(f"    -> ğŸ–¼ï¸ ë°ì´í„° URL ì²˜ë¦¬ ì‹œë„...")
            header, encoded = image_url.split(',', 1)
            image_content = base64.b64decode(encoded)
        else:
            print(f"    -> ğŸ–¼ï¸ ì›¹ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œë„: {image_url[:70]}...")
            response = requests.get(image_url, headers=headers, stream=True, timeout=15)
            response.raise_for_status()
            image_content = response.content

        if not image_content:
            return None

        img = Image.open(io.BytesIO(image_content))

        # ì´ë¯¸ì§€ ëª¨ë“œ ì²˜ë¦¬
        if img.mode in ('RGBA', 'LA'):
            print("    -> ğŸ’¡ íˆ¬ëª…ë„(PNG) ê°ì§€. í°ìƒ‰ ë°°ê²½ìœ¼ë¡œ ë³‘í•©í•©ë‹ˆë‹¤.")
            background = Image.new("RGB", img.size, (255, 255, 255))
            background.paste(img, (0, 0), img)
            img = background
        elif img.mode != 'RGB':
            print(f"    -> ğŸ’¡ ì´ë¯¸ì§€ ëª¨ë“œ({img.mode})ë¥¼ RGBë¡œ ë³€í™˜í•©ë‹ˆë‹¤.")
            img = img.convert('RGB')

        print(f"    -> ğŸ¤– Gemini OCR ì‹œë„...")
        response = OCR_MODEL.generate_content([OCR_PROMPT, img])
        time.sleep(1)

        extracted_text = response.text.strip()
        if extracted_text and len(extracted_text) > 5:
            print("    -> âœ… Gemini OCR ì„±ê³µ")
            return extracted_text
        else:
            print("    -> â„¹ï¸ Gemini OCR ê²°ê³¼ í…ìŠ¤íŠ¸ ì—†ìŒ")
            return None

    except Exception as e:
        print(f"    -> âŒ Gemini OCR ì²˜ë¦¬ ì‹¤íŒ¨! ì˜¤ë¥˜ íƒ€ì…: {e.__class__.__name__}, ë©”ì‹œì§€: {e}")
        return None


# ----------------------------
# 4. ë³¸ë¬¸ + ì´ë¯¸ì§€ + ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ í•¨ìˆ˜
# ----------------------------
def extract_content_from_soup(soup, url):
    try:
        title = soup.select_one('div.view_header h4').get_text(strip=True)

        content_area = soup.select_one('div.view_conts')
        main_text, image_urls = "", []

        if content_area:

            # ì´ë¯¸ì§€ ìˆ˜ì§‘
            for img_tag in content_area.select('img[src]'):
                image_urls.append(urljoin(url, img_tag['src']))

            # script/style ì œê±°
            for tag in content_area.find_all(['script', 'style']):
                tag.decompose()

            # ----------------------------
            # âš¡ ìƒˆ ë¡œì§: ë³¸ë¬¸ + í…Œì´ë¸”ì„ ì›ë˜ ìˆœì„œëŒ€ë¡œ ì¡°í•©
            # ----------------------------
            result_lines = []

            for elem in content_area.children:
                # í…ìŠ¤íŠ¸ ìš”ì†Œì¼ ê²½ìš°
                if elem.name is None:
                    text = elem.strip()
                    if text:
                        result_lines.append(text)

                # í…Œì´ë¸”ì¼ ê²½ìš° â†’ í…Œì´ë¸” íŒŒì‹±í•´ì„œ ì‚½ì…
                elif elem.name == "table":
                    table_rows = []
                    for tr in elem.find_all("tr"):
                        cols = []
                        for td in tr.find_all(["td", "th"]):
                            cell = td.get_text(separator=" ", strip=True)
                            cell = re.sub(r'\s+', ' ', cell)
                            cols.append(cell)
                        if cols:
                            table_rows.append(" | ".join(cols))
                    if table_rows:
                        result_lines.append("\n".join(table_rows))

                # p, div ë“± ë‹¤ë¥¸ íƒœê·¸ì˜ í…ìŠ¤íŠ¸ ì²˜ë¦¬
                else:
                    text = elem.get_text(separator="\n", strip=True)
                    if text:
                        result_lines.append(text)

            # ìµœì¢… main_text êµ¬ì„±
            main_text = "\n".join(result_lines)

        # ì²¨ë¶€íŒŒì¼
        attachments = []

        attachment_li = soup.select_one('li.attatch a[href]')
        if attachment_li:
            attachments.append({
                'filename': attachment_li.get_text(strip=True),
                'url': urljoin(url, attachment_li['href'])
            })

        file_list_area = soup.select_one('div.view_file')
        if file_list_area:
            for link_tag in file_list_area.select('a[href]'):
                attachments.append({
                    'filename': link_tag.get_text(strip=True),
                    'url': urljoin(url, link_tag['href'])
                })

        return title, main_text or 'ë³¸ë¬¸ ì—†ìŒ', image_urls, attachments

    except Exception as e:
        print(f"       -> âŒ ì›¹ í˜ì´ì§€ íŒŒì‹± ì˜¤ë¥˜: {e}")
        return "ì œëª© ì—†ìŒ", "ë³¸ë¬¸ ì—†ìŒ", [], []

   

# ----------------------------
# 5. í¬ë¡¤ë§ í•¨ìˆ˜
# ----------------------------
def start_crawling(start_url, headers, output_folder="cleaned_texts", max_pages=100):
    os.makedirs(output_folder, exist_ok=True)
    queue = deque([start_url])
    visited_urls = {start_url}
    page_count = 0

    base_netloc = urlparse(start_url).netloc
    allowed_patterns = ['acdnoti.php', 'notice.php']

    while queue and page_count < max_pages:
        current_url = queue.popleft()

        print(f"\nâ¡ï¸  í˜ì´ì§€ ë°©ë¬¸/ìˆ˜ì§‘: {current_url}")
        try:
            response = requests.get(current_url, headers=headers, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
        except Exception as e:
            print(f"    -> âŒ í˜ì´ì§€ ë°©ë¬¸ ì˜¤ë¥˜: {e}")
            continue

        # ----------------------------
        # ê²Œì‹œê¸€ ì½ê¸° ëª¨ë“œì¼ ë•Œ ì²˜ë¦¬
        # ----------------------------
        if 'mode=read' in current_url:
            title, main_text, image_urls, attachments = extract_content_from_soup(soup, current_url)
            print(f"    -> âœ… [{page_count + 1}/{max_pages}] ê²Œì‹œë¬¼ ì²˜ë¦¬: {title[:30]}...")

            ocr_texts = []
            for img_url in image_urls:
                if img_url and urlparse(img_url).scheme in ['http', 'https', 'data']:
                    ocr_result = ocr_with_gemini(img_url, headers)
                    if ocr_result:
                        ocr_texts.append(ocr_result)

            # OCR í…ìŠ¤íŠ¸ í¬í•¨
            full_content = main_text
            if ocr_texts:
                full_content += "\n\n--- ì´ë¯¸ì§€ ì¶”ì¶œ í…ìŠ¤íŠ¸ (Gemini OCR) ---\n"
                full_content += "\n".join(ocr_texts)

            final_text_to_save = f"ì¶œì²˜ URL: {current_url}\n"
            final_text_to_save += f"ì œëª©: {title}\n"

            # ---------- â˜… ì—¬ê¸° ìˆ˜ì •ë¨ (ë¬¸ë²• ì˜¤ë¥˜ ì œê±°) ----------
            if attachments:
                attachment_text = ";".join(
                    [f"{att['filename']}|{att['url']}" for att in attachments]
                )
                final_text_to_save += f"ì²¨ë¶€íŒŒì¼: {attachment_text}\n"
            # -----------------------------------------------------

            final_text_to_save += f"{'=' * 40}\n\n{clean_text(full_content)}"

            post_id_match = re.search(r'seq=(\d+)', current_url)
            if post_id_match:
                post_id = post_id_match.group(1)
                filename = f"kau_article_{post_id}.txt"
                try:
                    with open(os.path.join(output_folder, filename), 'w', encoding='utf-8') as f:
                        f.write(final_text_to_save)
                    page_count += 1
                except Exception as write_e:
                    print(f"    -> âŒ íŒŒì¼ ì“°ê¸° ì˜¤ë¥˜: {write_e}")
            else:
                print(f"    -> âš ï¸ ê²Œì‹œê¸€ ID(seq) ì—†ìŒ, íŒŒì¼ ì €ì¥ ìŠ¤í‚µ.")


        # ----------------------------
        # ë§í¬ ìˆ˜ì§‘
        # ----------------------------
        try:
            for link in soup.find_all('a', href=True):
                absolute_url = urljoin(current_url, link['href']).split('#')[0]

                if (
                    urlparse(absolute_url).netloc == base_netloc and
                    absolute_url not in visited_urls and
                    any(p in absolute_url for p in allowed_patterns)
                ):
                    visited_urls.add(absolute_url)
                    queue.append(absolute_url)

        except Exception as e:
            print(f"    -> âŒ ë§í¬ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")


# ----------------------------
# 6. ì‹¤í–‰
# ----------------------------
if __name__ == "__main__":
    start_url = 'https://kau.ac.kr/kaulife/acdnoti.php?searchkey=&searchvalue=&code=s1201&page=&mode=read&seq=9897'
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                      'AppleWebKit/537.36 (KHTML, like Gecko) '
                      'Chrome/129.0.0.0 Safari/537.36'
    }

    start_crawling(start_url, headers, max_pages=100)
    print("\n\ní¬ë¡¤ë§ ì™„ë£Œ.")
